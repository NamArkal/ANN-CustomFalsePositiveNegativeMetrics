{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as nm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as mpl\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm.random.seed(0)\n",
    "\n",
    "diabetic_retinopathy_dataset = pd.read_csv(\"/Users/namrata/Documents/Notes/ANN/DR Dataset.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      quality-assessment  pre-screening-result  MAD-1  MAD-2  MAD-3  MAD-4  \\\n",
      "0                      1                     1     22     22     22     19   \n",
      "1                      1                     1     24     24     22     18   \n",
      "2                      1                     1     62     60     59     54   \n",
      "3                      1                     1     55     53     53     50   \n",
      "4                      1                     1     44     44     44     41   \n",
      "5                      1                     1     44     43     41     41   \n",
      "6                      1                     0     29     29     29     27   \n",
      "7                      1                     1      6      6      6      6   \n",
      "8                      1                     1     22     21     18     15   \n",
      "9                      1                     1     79     75     73     71   \n",
      "10                     1                     1     45     45     45     43   \n",
      "11                     1                     0     25     25     25     23   \n",
      "12                     1                     1     70     69     65     63   \n",
      "13                     1                     1     48     43     39     32   \n",
      "14                     1                     1     94     93     92     89   \n",
      "15                     1                     1     20     18     16     15   \n",
      "16                     1                     1    105     95     81     66   \n",
      "17                     1                     1     25     25     24     23   \n",
      "18                     1                     1     64     64     63     58   \n",
      "19                     1                     0     46     41     39     32   \n",
      "20                     1                     1     37     37     37     34   \n",
      "21                     1                     1     19     17     15     12   \n",
      "22                     1                     0     37     34     31     30   \n",
      "23                     1                     1     10     10      9      9   \n",
      "24                     1                     1      5      5      5      5   \n",
      "25                     1                     1     40     38     33     25   \n",
      "26                     1                     1     55     53     51     47   \n",
      "27                     1                     1     99     98     68     53   \n",
      "28                     1                     1     45     45     45     43   \n",
      "29                     1                     1    103     89     83     71   \n",
      "...                  ...                   ...    ...    ...    ...    ...   \n",
      "1121                   1                     1     78     75     75     70   \n",
      "1122                   1                     1     30     30     29     27   \n",
      "1123                   1                     1      3      3      3      3   \n",
      "1124                   1                     1     25     24     24     23   \n",
      "1125                   1                     1     10     10     10     10   \n",
      "1126                   1                     1      4      4      4      4   \n",
      "1127                   1                     1     23     21     18     13   \n",
      "1128                   1                     1     21     21     21     20   \n",
      "1129                   1                     1     42     39     36     29   \n",
      "1130                   1                     1     37     36     33     24   \n",
      "1131                   1                     1     46     46     46     43   \n",
      "1132                   1                     1     86     84     83     82   \n",
      "1133                   1                     1     56     54     54     51   \n",
      "1134                   1                     1     77     71     69     61   \n",
      "1135                   1                     1     16     16     16     16   \n",
      "1136                   1                     1     65     60     48     31   \n",
      "1137                   1                     1     31     30     30     29   \n",
      "1138                   1                     1      3      3      3      3   \n",
      "1139                   1                     1     12     12     10      9   \n",
      "1140                   1                     1     31     31     31     27   \n",
      "1141                   1                     1     53     53     52     51   \n",
      "1142                   1                     1     53     53     51     47   \n",
      "1143                   1                     1     12     12     10      9   \n",
      "1144                   1                     1     35     34     33     33   \n",
      "1145                   1                     1     16     16     15     14   \n",
      "1146                   1                     1     34     34     34     33   \n",
      "1147                   1                     1     49     49     49     49   \n",
      "1148                   1                     0     49     48     48     45   \n",
      "1149                   1                     1     39     36     29     23   \n",
      "1150                   1                     1      7      7      7      7   \n",
      "\n",
      "      MAD-5  MAD-6   exudate-1  exudate-2  exudate-3  exudate-4  exudate-5  \\\n",
      "0        18     14   49.895756  17.775994   5.270920   0.771761   0.018632   \n",
      "1        16     13   57.709936  23.799994   3.325423   0.234185   0.003903   \n",
      "2        47     33   55.831441  27.993933  12.687485   4.852282   1.393889   \n",
      "3        43     31   40.467228  18.445954   9.118901   3.079428   0.840261   \n",
      "4        39     27   18.026254   8.570709   0.410381   0.000000   0.000000   \n",
      "5        37     29   28.356400   6.935636   2.305771   0.323724   0.000000   \n",
      "6        25     16   15.448398   9.113819   1.633493   0.000000   0.000000   \n",
      "7         2      1   20.679649   9.497786   1.223660   0.150382   0.000000   \n",
      "8        13     10   66.691933  23.545543   6.151117   0.496372   0.000000   \n",
      "9        64     47   22.141784  10.054384   0.874633   0.099780   0.023386   \n",
      "10       40     32   84.358401  50.977459  17.293722   1.974419   0.000000   \n",
      "11       22     18   22.480047  13.949995   0.436232   0.116119   0.000000   \n",
      "12       63     50   10.560100   3.108358   0.625511   0.287959   0.103985   \n",
      "13       27     18   23.012798   6.737583   2.403903   0.189235   0.011437   \n",
      "14       86     77    8.610822   1.981319   0.401183   0.066095   0.000000   \n",
      "15       13      9   65.113664  33.124797   8.785379   0.673542   0.051811   \n",
      "16       46     32  123.053484  70.571010  37.409891  19.937253  14.786668   \n",
      "17       22     19   17.034060   9.976938   1.067243   0.484829   0.467790   \n",
      "18       55     40   19.673459   6.064866   0.907342   0.080105   0.000000   \n",
      "19       23     15  115.533777  21.293312   9.665742   2.276676   0.329396   \n",
      "20       31     23   61.357614  35.165912   8.114027   1.204871   0.178499   \n",
      "21       12      7  179.703958  34.678202  13.018953   1.045157   0.023003   \n",
      "22       28     24    8.818234   3.161544   1.900918   1.524727   1.292870   \n",
      "23        9      6   72.938941  20.285362   9.793215   0.916265   0.040814   \n",
      "24        4      3  133.054234   6.890885   2.718365   0.169898   0.017940   \n",
      "25       20     12   73.082699  23.121256  13.093588   5.437382   3.845287   \n",
      "26       39     26   71.337302  39.430203  21.117569   3.956318   0.636703   \n",
      "27       42     27  298.068510  50.269654  33.693698  10.989180   6.124441   \n",
      "28       37     24   35.173512  15.421144   5.206171   1.491918   0.371425   \n",
      "29       60     38   11.025085   3.762343   0.015592   0.010914   0.003118   \n",
      "...     ...    ...         ...        ...        ...        ...        ...   \n",
      "1121     65     43   15.259422   8.751638   4.265453   0.185719   0.039580   \n",
      "1122     23     17  140.399520  66.310840  23.107818   3.045977   0.043178   \n",
      "1123      3      2   26.240091  14.503978   1.847966   0.107202   0.000000   \n",
      "1124     18     14   83.135316  46.049413  11.415154   1.663975   0.066478   \n",
      "1125      8      6   40.641723  19.202111   4.663533   0.399850   0.000000   \n",
      "1126      4      3  118.036826  21.578380   7.407548   0.725415   0.003959   \n",
      "1127      7      4   35.347026   9.431590   1.661299   0.247955   0.191132   \n",
      "1128     20     16    5.121229   1.694872   0.280699   0.000000   0.000000   \n",
      "1129     21     13   96.385981  24.281414   7.722666   0.641116   0.242004   \n",
      "1130     18     10   37.686807   4.978201   1.180336   0.184896   0.024986   \n",
      "1131     42     34   26.839906  16.254480   5.512953   0.316836   0.109734   \n",
      "1132     80     63   24.226598   4.989495   1.692755   0.165296   0.009183   \n",
      "1133     48     36    8.804178   1.918597   0.168133   0.000000   0.000000   \n",
      "1134     39     25  159.904908  94.032005  42.230588  16.562221   8.562842   \n",
      "1135     15      9  182.410841  51.850020  19.226295   3.303264   0.344026   \n",
      "1136     24     13  159.723711  43.983882  17.081478   1.644218   0.248323   \n",
      "1137     28     21   23.736589  12.865507   1.036667   0.072721   0.015473   \n",
      "1138      1      1   22.266376   6.890557   0.731205   0.063583   0.007179   \n",
      "1139      8      4   51.440209  26.643732   6.511260   0.312865   0.000000   \n",
      "1140     22     17   22.404714  13.085926   1.196279   0.100579   0.000000   \n",
      "1141     51     41    8.632010   3.852405   0.383992   0.290335   0.165460   \n",
      "1142     44     32   25.283836   8.337976   0.720774   0.036805   0.000000   \n",
      "1143      7      6  243.066702  26.650463  13.732893   0.099581   0.002075   \n",
      "1144     33     26    2.579859   0.001552   0.000000   0.000000   0.000000   \n",
      "1145     12      8  158.177307  84.865487  51.253664  11.283321   1.857293   \n",
      "1146     31     24    6.071765   0.937472   0.031145   0.003115   0.000000   \n",
      "1147     45     37   63.197145  27.377668   8.067688   0.979548   0.001552   \n",
      "1148     43     33   30.461898  13.966980   1.763305   0.137858   0.011221   \n",
      "1149     13      7   40.525739  12.604947   4.740919   1.077570   0.563518   \n",
      "1150      7      5   69.423565   7.031843   1.750548   0.046597   0.021180   \n",
      "\n",
      "      exudate-6  exudate-7  exudate-8  ROI-diameter  optic-disc-diameter  \\\n",
      "0      0.006864   0.003923   0.003923      0.486903             0.100025   \n",
      "1      0.003903   0.003903   0.003903      0.520908             0.144414   \n",
      "2      0.373252   0.041817   0.007744      0.530904             0.128548   \n",
      "3      0.272434   0.007653   0.001531      0.483284             0.114790   \n",
      "4      0.000000   0.000000   0.000000      0.475935             0.123572   \n",
      "5      0.000000   0.000000   0.000000      0.502831             0.126741   \n",
      "6      0.000000   0.000000   0.000000      0.541743             0.139575   \n",
      "7      0.000000   0.000000   0.000000      0.576318             0.071071   \n",
      "8      0.000000   0.000000   0.000000      0.500073             0.116793   \n",
      "9      0.000000   0.000000   0.000000      0.560959             0.109134   \n",
      "10     0.000000   0.000000   0.000000      0.546008             0.112378   \n",
      "11     0.000000   0.000000   0.000000      0.551682             0.139657   \n",
      "12     0.004799   0.000000   0.000000      0.534396             0.089587   \n",
      "13     0.000000   0.000000   0.000000      0.501554             0.138287   \n",
      "14     0.000000   0.000000   0.000000      0.541277             0.124505   \n",
      "15     0.002933   0.000978   0.000978      0.569458             0.089936   \n",
      "16     6.114911   2.345740   1.002243      0.524461             0.134247   \n",
      "17     0.306697   0.188975   0.130114      0.552002             0.108428   \n",
      "18     0.000000   0.000000   0.000000      0.551182             0.098591   \n",
      "19     0.186000   0.118458   0.071698      0.540472             0.104949   \n",
      "20     0.010772   0.000000   0.000000      0.478189             0.110793   \n",
      "21     0.005001   0.002000   0.000000      0.470425             0.094014   \n",
      "22     0.165831   0.000000   0.000000      0.538223             0.098270   \n",
      "23     0.000000   0.000000   0.000000      0.528929             0.108156   \n",
      "24     0.011608   0.003166   0.001055      0.588627             0.109748   \n",
      "25     2.028783   0.518568   0.107150      0.527112             0.105129   \n",
      "26     0.032852   0.000000   0.000000      0.540769             0.117329   \n",
      "27     1.748958   0.358444   0.181282      0.556481             0.117421   \n",
      "28     0.017095   0.000000   0.000000      0.543355             0.118110   \n",
      "29     0.003118   0.001559   0.001559      0.488566             0.134091   \n",
      "...         ...        ...        ...           ...                  ...   \n",
      "1121   0.004567   0.000000   0.000000      0.535677             0.118739   \n",
      "1122   0.003925   0.003925   0.003925      0.506181             0.095187   \n",
      "1123   0.000000   0.000000   0.000000      0.480484             0.113328   \n",
      "1124   0.009065   0.000000   0.000000      0.476429             0.131950   \n",
      "1125   0.000000   0.000000   0.000000      0.549111             0.120162   \n",
      "1126   0.003959   0.003959   0.003959      0.508743             0.105893   \n",
      "1127   0.169436   0.139475   0.001033      0.494143             0.099182   \n",
      "1128   0.000000   0.000000   0.000000      0.476500             0.073226   \n",
      "1129   0.100510   0.072211   0.017565      0.487241             0.084897   \n",
      "1130   0.007996   0.000000   0.000000      0.535271             0.093947   \n",
      "1131   0.000000   0.000000   0.000000      0.520977             0.095824   \n",
      "1132   0.003061   0.003061   0.000000      0.537999             0.097953   \n",
      "1133   0.000000   0.000000   0.000000      0.529728             0.100565   \n",
      "1134   5.137705   2.191362   1.327518      0.530547             0.127006   \n",
      "1135   0.085491   0.019570   0.013390      0.514502             0.104032   \n",
      "1136   0.087706   0.065515   0.051778      0.558896             0.127860   \n",
      "1137   0.000000   0.000000   0.000000      0.516223             0.120687   \n",
      "1138   0.001026   0.000000   0.000000      0.513383             0.106655   \n",
      "1139   0.000000   0.000000   0.000000      0.520232             0.100426   \n",
      "1140   0.000000   0.000000   0.000000      0.555720             0.109722   \n",
      "1141   0.109266   0.085852   0.071803      0.551013             0.087413   \n",
      "1142   0.000000   0.000000   0.000000      0.471802             0.141088   \n",
      "1143   0.001037   0.000000   0.000000      0.553246             0.084022   \n",
      "1144   0.000000   0.000000   0.000000      0.537551             0.124181   \n",
      "1145   0.019520   0.006832   0.001952      0.533177             0.104430   \n",
      "1146   0.000000   0.000000   0.000000      0.537470             0.116795   \n",
      "1147   0.000000   0.000000   0.000000      0.516733             0.124190   \n",
      "1148   0.000000   0.000000   0.000000      0.560632             0.129843   \n",
      "1149   0.326860   0.239568   0.174584      0.485972             0.106690   \n",
      "1150   0.008472   0.000000   0.000000      0.556192             0.088957   \n",
      "\n",
      "      AM-FM  result  \n",
      "0         1       0  \n",
      "1         0       0  \n",
      "2         0       1  \n",
      "3         0       0  \n",
      "4         0       1  \n",
      "5         0       1  \n",
      "6         0       1  \n",
      "7         1       0  \n",
      "8         0       1  \n",
      "9         0       1  \n",
      "10        0       0  \n",
      "11        1       0  \n",
      "12        0       1  \n",
      "13        1       1  \n",
      "14        0       0  \n",
      "15        1       0  \n",
      "16        1       1  \n",
      "17        0       0  \n",
      "18        0       0  \n",
      "19        1       1  \n",
      "20        0       0  \n",
      "21        1       1  \n",
      "22        0       1  \n",
      "23        1       0  \n",
      "24        1       1  \n",
      "25        1       1  \n",
      "26        0       0  \n",
      "27        1       1  \n",
      "28        0       0  \n",
      "29        0       1  \n",
      "...     ...     ...  \n",
      "1121      0       1  \n",
      "1122      1       0  \n",
      "1123      1       0  \n",
      "1124      0       1  \n",
      "1125      0       0  \n",
      "1126      1       0  \n",
      "1127      1       1  \n",
      "1128      1       1  \n",
      "1129      0       1  \n",
      "1130      0       1  \n",
      "1131      0       1  \n",
      "1132      0       1  \n",
      "1133      0       1  \n",
      "1134      1       1  \n",
      "1135      1       0  \n",
      "1136      1       1  \n",
      "1137      0       1  \n",
      "1138      0       1  \n",
      "1139      0       1  \n",
      "1140      0       1  \n",
      "1141      1       1  \n",
      "1142      0       0  \n",
      "1143      1       1  \n",
      "1144      0       0  \n",
      "1145      1       0  \n",
      "1146      0       0  \n",
      "1147      0       0  \n",
      "1148      0       0  \n",
      "1149      1       1  \n",
      "1150      0       0  \n",
      "\n",
      "[1151 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "print(diabetic_retinopathy_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = diabetic_retinopathy_dataset.drop(\"result\", axis =1)\n",
    "y = diabetic_retinopathy_dataset.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = pd.DataFrame(data=scaler.transform(x_train), columns = x_train.columns, index = x_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = nm.array(x_train)\n",
    "y_train = nm.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler()\n",
    "scaler2.fit(x_test)\n",
    "x_test = pd.DataFrame(data=scaler2.transform(x_test), columns=x_test.columns, index=x_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = nm.array(x_test)\n",
    "y_test = nm.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(32, input_dim=19, activation='relu'))\n",
    "network.add(layers.Dense(16, input_dim=32, activation='relu'))\n",
    "network.add(layers.Dense(1, activation='sigmoid'))\n",
    "sgd = optimizers.SGD(lr = 0.10)\n",
    "network.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "805/805 [==============================] - 1s 1ms/step - loss: 0.7187 - acc: 0.5143\n",
      "Epoch 2/50\n",
      "805/805 [==============================] - 0s 107us/step - loss: 0.6792 - acc: 0.5217\n",
      "Epoch 3/50\n",
      "805/805 [==============================] - 0s 107us/step - loss: 0.6625 - acc: 0.5814\n",
      "Epoch 4/50\n",
      "805/805 [==============================] - 0s 108us/step - loss: 0.6533 - acc: 0.6186\n",
      "Epoch 5/50\n",
      "805/805 [==============================] - 0s 107us/step - loss: 0.6465 - acc: 0.6323\n",
      "Epoch 6/50\n",
      "805/805 [==============================] - 0s 108us/step - loss: 0.6406 - acc: 0.6360\n",
      "Epoch 7/50\n",
      "805/805 [==============================] - 0s 105us/step - loss: 0.6355 - acc: 0.6398\n",
      "Epoch 8/50\n",
      "805/805 [==============================] - 0s 106us/step - loss: 0.6309 - acc: 0.6460\n",
      "Epoch 9/50\n",
      "805/805 [==============================] - 0s 109us/step - loss: 0.6266 - acc: 0.6509\n",
      "Epoch 10/50\n",
      "805/805 [==============================] - 0s 107us/step - loss: 0.6226 - acc: 0.6621\n",
      "Epoch 11/50\n",
      "805/805 [==============================] - 0s 105us/step - loss: 0.6186 - acc: 0.6658\n",
      "Epoch 12/50\n",
      "805/805 [==============================] - 0s 105us/step - loss: 0.6147 - acc: 0.6671\n",
      "Epoch 13/50\n",
      "805/805 [==============================] - 0s 110us/step - loss: 0.6112 - acc: 0.6708\n",
      "Epoch 14/50\n",
      "805/805 [==============================] - 0s 110us/step - loss: 0.6078 - acc: 0.6671\n",
      "Epoch 15/50\n",
      "805/805 [==============================] - 0s 105us/step - loss: 0.6044 - acc: 0.6671\n",
      "Epoch 16/50\n",
      "805/805 [==============================] - 0s 103us/step - loss: 0.6010 - acc: 0.6733\n",
      "Epoch 17/50\n",
      "805/805 [==============================] - 0s 104us/step - loss: 0.5983 - acc: 0.6770\n",
      "Epoch 18/50\n",
      "805/805 [==============================] - 0s 105us/step - loss: 0.5952 - acc: 0.6783\n",
      "Epoch 19/50\n",
      "805/805 [==============================] - 0s 106us/step - loss: 0.5923 - acc: 0.6795\n",
      "Epoch 20/50\n",
      "805/805 [==============================] - 0s 108us/step - loss: 0.5896 - acc: 0.6733\n",
      "Epoch 21/50\n",
      "805/805 [==============================] - 0s 109us/step - loss: 0.5868 - acc: 0.6807\n",
      "Epoch 22/50\n",
      "805/805 [==============================] - 0s 110us/step - loss: 0.5842 - acc: 0.6845\n",
      "Epoch 23/50\n",
      "805/805 [==============================] - 0s 107us/step - loss: 0.5816 - acc: 0.6919\n",
      "Epoch 24/50\n",
      "805/805 [==============================] - 0s 109us/step - loss: 0.5793 - acc: 0.6932\n",
      "Epoch 25/50\n",
      "805/805 [==============================] - 0s 105us/step - loss: 0.5768 - acc: 0.6994\n",
      "Epoch 26/50\n",
      "805/805 [==============================] - 0s 107us/step - loss: 0.5746 - acc: 0.7019\n",
      "Epoch 27/50\n",
      "805/805 [==============================] - 0s 106us/step - loss: 0.5722 - acc: 0.6957\n",
      "Epoch 28/50\n",
      "805/805 [==============================] - 0s 134us/step - loss: 0.5697 - acc: 0.7031\n",
      "Epoch 29/50\n",
      "805/805 [==============================] - 0s 147us/step - loss: 0.5678 - acc: 0.7056\n",
      "Epoch 30/50\n",
      "805/805 [==============================] - 0s 118us/step - loss: 0.5654 - acc: 0.7093\n",
      "Epoch 31/50\n",
      "805/805 [==============================] - 0s 107us/step - loss: 0.5635 - acc: 0.7106\n",
      "Epoch 32/50\n",
      "805/805 [==============================] - 0s 113us/step - loss: 0.5613 - acc: 0.7193\n",
      "Epoch 33/50\n",
      "805/805 [==============================] - 0s 134us/step - loss: 0.5589 - acc: 0.7130\n",
      "Epoch 34/50\n",
      "805/805 [==============================] - 0s 144us/step - loss: 0.5570 - acc: 0.7180\n",
      "Epoch 35/50\n",
      "805/805 [==============================] - 0s 133us/step - loss: 0.5551 - acc: 0.7230\n",
      "Epoch 36/50\n",
      "805/805 [==============================] - 0s 121us/step - loss: 0.5532 - acc: 0.7217\n",
      "Epoch 37/50\n",
      "805/805 [==============================] - 0s 134us/step - loss: 0.5512 - acc: 0.7230\n",
      "Epoch 38/50\n",
      "805/805 [==============================] - 0s 147us/step - loss: 0.5495 - acc: 0.7205\n",
      "Epoch 39/50\n",
      "805/805 [==============================] - 0s 115us/step - loss: 0.5476 - acc: 0.7242\n",
      "Epoch 40/50\n",
      "805/805 [==============================] - 0s 112us/step - loss: 0.5459 - acc: 0.7267\n",
      "Epoch 41/50\n",
      "805/805 [==============================] - 0s 128us/step - loss: 0.5441 - acc: 0.7366\n",
      "Epoch 42/50\n",
      "805/805 [==============================] - 0s 147us/step - loss: 0.5427 - acc: 0.7342\n",
      "Epoch 43/50\n",
      "805/805 [==============================] - 0s 123us/step - loss: 0.5409 - acc: 0.7354\n",
      "Epoch 44/50\n",
      "805/805 [==============================] - 0s 109us/step - loss: 0.5394 - acc: 0.7329\n",
      "Epoch 45/50\n",
      "805/805 [==============================] - 0s 116us/step - loss: 0.5379 - acc: 0.7354\n",
      "Epoch 46/50\n",
      "805/805 [==============================] - 0s 148us/step - loss: 0.5361 - acc: 0.7329\n",
      "Epoch 47/50\n",
      "805/805 [==============================] - 0s 127us/step - loss: 0.5347 - acc: 0.7379\n",
      "Epoch 48/50\n",
      "805/805 [==============================] - 0s 109us/step - loss: 0.5333 - acc: 0.7404\n",
      "Epoch 49/50\n",
      "805/805 [==============================] - 0s 109us/step - loss: 0.5318 - acc: 0.7379\n",
      "Epoch 50/50\n",
      "805/805 [==============================] - 0s 113us/step - loss: 0.5305 - acc: 0.7366\n"
     ]
    }
   ],
   "source": [
    "training = network.fit(x_train, y_train, epochs=50, batch_size=10, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_pred):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_pred)): \n",
    "        if y_actual[i]==y_pred[i]==1:\n",
    "           TP += 1\n",
    "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_pred[i]==0:\n",
    "           TN += 1\n",
    "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "346/346 [==============================] - 0s 98us/step - loss: 0.4976 - acc: 0.7428\n",
      "Epoch 2/50\n",
      "346/346 [==============================] - 0s 99us/step - loss: 0.4978 - acc: 0.7486\n",
      "Epoch 3/50\n",
      "346/346 [==============================] - 0s 113us/step - loss: 0.4976 - acc: 0.7457\n",
      "Epoch 4/50\n",
      "346/346 [==============================] - 0s 98us/step - loss: 0.4972 - acc: 0.7428\n",
      "Epoch 5/50\n",
      "346/346 [==============================] - 0s 94us/step - loss: 0.4970 - acc: 0.7457\n",
      "Epoch 6/50\n",
      "346/346 [==============================] - 0s 99us/step - loss: 0.4971 - acc: 0.7428\n",
      "Epoch 7/50\n",
      "346/346 [==============================] - 0s 95us/step - loss: 0.4970 - acc: 0.7543\n",
      "Epoch 8/50\n",
      "346/346 [==============================] - 0s 94us/step - loss: 0.4954 - acc: 0.7457\n",
      "Epoch 9/50\n",
      "346/346 [==============================] - 0s 94us/step - loss: 0.4960 - acc: 0.7514\n",
      "Epoch 10/50\n",
      "346/346 [==============================] - 0s 96us/step - loss: 0.4952 - acc: 0.7630\n",
      "Epoch 11/50\n",
      "346/346 [==============================] - 0s 96us/step - loss: 0.4939 - acc: 0.7572\n",
      "Epoch 12/50\n",
      "346/346 [==============================] - 0s 95us/step - loss: 0.4929 - acc: 0.7514\n",
      "Epoch 13/50\n",
      "346/346 [==============================] - 0s 96us/step - loss: 0.4958 - acc: 0.7486\n",
      "Epoch 14/50\n",
      "346/346 [==============================] - 0s 94us/step - loss: 0.4949 - acc: 0.7514\n",
      "Epoch 15/50\n",
      "346/346 [==============================] - 0s 94us/step - loss: 0.4939 - acc: 0.7543\n",
      "Epoch 16/50\n",
      "346/346 [==============================] - 0s 96us/step - loss: 0.4935 - acc: 0.7514\n",
      "Epoch 17/50\n",
      "346/346 [==============================] - 0s 101us/step - loss: 0.4926 - acc: 0.7543\n",
      "Epoch 18/50\n",
      "346/346 [==============================] - 0s 104us/step - loss: 0.4928 - acc: 0.7630\n",
      "Epoch 19/50\n",
      "346/346 [==============================] - 0s 100us/step - loss: 0.4925 - acc: 0.7601\n",
      "Epoch 20/50\n",
      "346/346 [==============================] - 0s 101us/step - loss: 0.4922 - acc: 0.7601\n",
      "Epoch 21/50\n",
      "346/346 [==============================] - 0s 105us/step - loss: 0.4927 - acc: 0.7630\n",
      "Epoch 22/50\n",
      "346/346 [==============================] - 0s 100us/step - loss: 0.4903 - acc: 0.7630\n",
      "Epoch 23/50\n",
      "346/346 [==============================] - 0s 90us/step - loss: 0.4916 - acc: 0.7572\n",
      "Epoch 24/50\n",
      "346/346 [==============================] - 0s 86us/step - loss: 0.4915 - acc: 0.7543\n",
      "Epoch 25/50\n",
      "346/346 [==============================] - 0s 89us/step - loss: 0.4911 - acc: 0.7514\n",
      "Epoch 26/50\n",
      "346/346 [==============================] - 0s 94us/step - loss: 0.4903 - acc: 0.7543\n",
      "Epoch 27/50\n",
      "346/346 [==============================] - 0s 91us/step - loss: 0.4895 - acc: 0.7659\n",
      "Epoch 28/50\n",
      "346/346 [==============================] - 0s 91us/step - loss: 0.4911 - acc: 0.7543\n",
      "Epoch 29/50\n",
      "346/346 [==============================] - 0s 90us/step - loss: 0.4895 - acc: 0.7514\n",
      "Epoch 30/50\n",
      "346/346 [==============================] - 0s 92us/step - loss: 0.4877 - acc: 0.7601\n",
      "Epoch 31/50\n",
      "346/346 [==============================] - 0s 90us/step - loss: 0.4890 - acc: 0.7572\n",
      "Epoch 32/50\n",
      "346/346 [==============================] - 0s 89us/step - loss: 0.4872 - acc: 0.7601\n",
      "Epoch 33/50\n",
      "346/346 [==============================] - 0s 88us/step - loss: 0.4872 - acc: 0.7572\n",
      "Epoch 34/50\n",
      "346/346 [==============================] - 0s 82us/step - loss: 0.4871 - acc: 0.7601\n",
      "Epoch 35/50\n",
      "346/346 [==============================] - 0s 92us/step - loss: 0.4876 - acc: 0.7601\n",
      "Epoch 36/50\n",
      "346/346 [==============================] - 0s 88us/step - loss: 0.4873 - acc: 0.7572\n",
      "Epoch 37/50\n",
      "346/346 [==============================] - 0s 88us/step - loss: 0.4859 - acc: 0.7543\n",
      "Epoch 38/50\n",
      "346/346 [==============================] - 0s 87us/step - loss: 0.4868 - acc: 0.7514\n",
      "Epoch 39/50\n",
      "346/346 [==============================] - 0s 87us/step - loss: 0.4852 - acc: 0.7572\n",
      "Epoch 40/50\n",
      "346/346 [==============================] - 0s 91us/step - loss: 0.4841 - acc: 0.7572\n",
      "Epoch 41/50\n",
      "346/346 [==============================] - 0s 89us/step - loss: 0.4854 - acc: 0.7630\n",
      "Epoch 42/50\n",
      "346/346 [==============================] - 0s 93us/step - loss: 0.4851 - acc: 0.7601\n",
      "Epoch 43/50\n",
      "346/346 [==============================] - 0s 105us/step - loss: 0.4839 - acc: 0.7659\n",
      "Epoch 44/50\n",
      "346/346 [==============================] - 0s 110us/step - loss: 0.4852 - acc: 0.7630\n",
      "Epoch 45/50\n",
      "346/346 [==============================] - 0s 114us/step - loss: 0.4831 - acc: 0.7659\n",
      "Epoch 46/50\n",
      "346/346 [==============================] - 0s 113us/step - loss: 0.4831 - acc: 0.7572\n",
      "Epoch 47/50\n",
      "346/346 [==============================] - 0s 111us/step - loss: 0.4830 - acc: 0.7688\n",
      "Epoch 48/50\n",
      "346/346 [==============================] - 0s 111us/step - loss: 0.4829 - acc: 0.7659\n",
      "Epoch 49/50\n",
      "346/346 [==============================] - 0s 101us/step - loss: 0.4814 - acc: 0.7659\n",
      "Epoch 50/50\n",
      "346/346 [==============================] - 0s 100us/step - loss: 0.4808 - acc: 0.7514\n"
     ]
    }
   ],
   "source": [
    "testing = network.fit(x_test, y_test, epochs=50, batch_size=10, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = network.predict_classes(x_train)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rates(tp, fp, tn, fn):\n",
    "    FPR = 0\n",
    "    FNR = 0\n",
    "\n",
    "    FNR = fn/(fn+tp)\n",
    "    FPR = fp/(fp+tn)\n",
    "\n",
    "    return(FPR, FNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(285, 80, 304, 136)\n"
     ]
    }
   ],
   "source": [
    "confMatrix = perf_measure(y_train, y_pred)\n",
    "print(confMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.20833333333333334, 0.32304038004750596)\n"
     ]
    }
   ],
   "source": [
    "tp = confMatrix[0]\n",
    "fp = confMatrix[1]\n",
    "tn = confMatrix[2]\n",
    "fn = confMatrix[3]\n",
    "bpRates = rates(tp, fp, tn, fn)\n",
    "print(bpRates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "actual_values = confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.19010416666666666, 0.25653206650831356)\n"
     ]
    }
   ],
   "source": [
    "tn1 = actual_values[0][0]\n",
    "fp1 = actual_values[0][1]\n",
    "fn1 = actual_values[1][0]\n",
    "tp1 = actual_values[1][1]\n",
    "newBpRates = rates(tp1, fp1, tn1, fn1)\n",
    "print(newBpRates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def falsePositiveRate(threshold=0.5):\n",
    "    def FPR(y_true, y_pred):\n",
    "        threshold_value = threshold\n",
    "        y_pred = K.cast(K.less(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n",
    "        true_negatives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n",
    "#         true_negatives = K.sum(K.clip(y_true * y_pred, 0, 1))\n",
    "        predicted_negatives = K.sum(y_pred)\n",
    "        precision_ratio = true_negatives / (predicted_negatives + K.epsilon())\n",
    "        return ((1-precision_ratio)*100)\n",
    "#         return (1-precision_ratio)\n",
    "    return FPR\n",
    "\n",
    "def falseNegativeRate(threshold = 0.5):\n",
    "    def FNR(y_true, y_pred):\n",
    "        threshold_value = threshold\n",
    "        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n",
    "        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n",
    "#         true_positives = K.sum(K.clip(y_true * y_pred, 0, 1))\n",
    "        possible_positives = K.sum(K.clip(y_true, 0, 1))\n",
    "        recall_ratio = true_positives / (possible_positives + K.epsilon())\n",
    "        return ((1-recall_ratio)*100)\n",
    "#         return (1-recall_ratio)\n",
    "    return FNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy',falsePositiveRate(0.9),falseNegativeRate(0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "805/805 [==============================] - 1s 2ms/step - loss: 0.3353 - acc: 0.8472 - FPR: 66.0569 - FNR: 1.0056\n",
      "Epoch 2/50\n",
      "805/805 [==============================] - 0s 122us/step - loss: 0.3415 - acc: 0.8435 - FPR: 65.6551 - FNR: 1.5247\n",
      "Epoch 3/50\n",
      "805/805 [==============================] - 0s 123us/step - loss: 0.3426 - acc: 0.8422 - FPR: 66.6972 - FNR: 1.0796\n",
      "Epoch 4/50\n",
      "805/805 [==============================] - 0s 124us/step - loss: 0.3361 - acc: 0.8534 - FPR: 66.3290 - FNR: 1.1727\n",
      "Epoch 5/50\n",
      "805/805 [==============================] - 0s 124us/step - loss: 0.3370 - acc: 0.8596 - FPR: 66.1358 - FNR: 2.7669\n",
      "Epoch 6/50\n",
      "805/805 [==============================] - 0s 131us/step - loss: 0.3356 - acc: 0.8522 - FPR: 66.3251 - FNR: 1.6681\n",
      "Epoch 7/50\n",
      "805/805 [==============================] - 0s 161us/step - loss: 0.3393 - acc: 0.8373 - FPR: 66.1417 - FNR: 0.8518\n",
      "Epoch 8/50\n",
      "805/805 [==============================] - 0s 156us/step - loss: 0.3380 - acc: 0.8435 - FPR: 65.7882 - FNR: 0.9588\n",
      "Epoch 9/50\n",
      "805/805 [==============================] - 0s 150us/step - loss: 0.3361 - acc: 0.8410 - FPR: 64.9606 - FNR: 1.1077\n",
      "Epoch 10/50\n",
      "805/805 [==============================] - 0s 150us/step - loss: 0.3354 - acc: 0.8522 - FPR: 65.2741 - FNR: 1.3073\n",
      "Epoch 11/50\n",
      "805/805 [==============================] - 0s 142us/step - loss: 0.3387 - acc: 0.8435 - FPR: 66.2013 - FNR: 0.6329\n",
      "Epoch 12/50\n",
      "805/805 [==============================] - 0s 126us/step - loss: 0.3361 - acc: 0.8559 - FPR: 65.1425 - FNR: 1.0766\n",
      "Epoch 13/50\n",
      "805/805 [==============================] - 0s 124us/step - loss: 0.3383 - acc: 0.8447 - FPR: 65.4791 - FNR: 2.1754\n",
      "Epoch 14/50\n",
      "805/805 [==============================] - 0s 122us/step - loss: 0.3398 - acc: 0.8509 - FPR: 65.0404 - FNR: 0.8710\n",
      "Epoch 15/50\n",
      "805/805 [==============================] - 0s 124us/step - loss: 0.3356 - acc: 0.8373 - FPR: 65.3618 - FNR: 2.3810\n",
      "Epoch 16/50\n",
      "805/805 [==============================] - 0s 166us/step - loss: 0.3370 - acc: 0.8584 - FPR: 66.3980 - FNR: 1.2334\n",
      "Epoch 17/50\n",
      "805/805 [==============================] - 0s 155us/step - loss: 0.3372 - acc: 0.8509 - FPR: 65.3431 - FNR: 2.5377\n",
      "Epoch 18/50\n",
      "805/805 [==============================] - 0s 127us/step - loss: 0.3376 - acc: 0.8534 - FPR: 65.8715 - FNR: 2.5412\n",
      "Epoch 19/50\n",
      "805/805 [==============================] - 0s 127us/step - loss: 0.3354 - acc: 0.8472 - FPR: 66.7406 - FNR: 1.5750\n",
      "Epoch 20/50\n",
      "805/805 [==============================] - 0s 166us/step - loss: 0.3369 - acc: 0.8472 - FPR: 66.6110 - FNR: 0.5176\n",
      "Epoch 21/50\n",
      "805/805 [==============================] - 0s 161us/step - loss: 0.3357 - acc: 0.8484 - FPR: 65.7168 - FNR: 0.9539\n",
      "Epoch 22/50\n",
      "805/805 [==============================] - 0s 142us/step - loss: 0.3388 - acc: 0.8534 - FPR: 65.7123 - FNR: 1.3073\n",
      "Epoch 23/50\n",
      "805/805 [==============================] - 0s 164us/step - loss: 0.3310 - acc: 0.8534 - FPR: 66.3142 - FNR: 1.6770\n",
      "Epoch 24/50\n",
      "805/805 [==============================] - 0s 155us/step - loss: 0.3382 - acc: 0.8360 - FPR: 66.0879 - FNR: 1.0352\n",
      "Epoch 25/50\n",
      "805/805 [==============================] - 0s 149us/step - loss: 0.3355 - acc: 0.8497 - FPR: 66.6198 - FNR: 2.0497\n",
      "Epoch 26/50\n",
      "805/805 [==============================] - 0s 153us/step - loss: 0.3364 - acc: 0.8447 - FPR: 65.9751 - FNR: 1.3177\n",
      "Epoch 27/50\n",
      "805/805 [==============================] - 0s 162us/step - loss: 0.3337 - acc: 0.8435 - FPR: 65.4708 - FNR: 0.8725\n",
      "Epoch 28/50\n",
      "805/805 [==============================] - 0s 137us/step - loss: 0.3371 - acc: 0.8447 - FPR: 66.0229 - FNR: 1.6164\n",
      "Epoch 29/50\n",
      "805/805 [==============================] - 0s 125us/step - loss: 0.3343 - acc: 0.8497 - FPR: 65.6679 - FNR: 0.6970\n",
      "Epoch 30/50\n",
      "805/805 [==============================] - 0s 124us/step - loss: 0.3405 - acc: 0.8522 - FPR: 65.5344 - FNR: 1.8525\n",
      "Epoch 31/50\n",
      "805/805 [==============================] - 0s 124us/step - loss: 0.3373 - acc: 0.8509 - FPR: 65.8646 - FNR: 2.5998\n",
      "Epoch 32/50\n",
      "805/805 [==============================] - 0s 161us/step - loss: 0.3366 - acc: 0.8497 - FPR: 65.7557 - FNR: 0.7468\n",
      "Epoch 33/50\n",
      "805/805 [==============================] - 0s 166us/step - loss: 0.3325 - acc: 0.8497 - FPR: 65.7636 - FNR: 0.9243\n",
      "Epoch 34/50\n",
      "805/805 [==============================] - 0s 157us/step - loss: 0.3361 - acc: 0.8472 - FPR: 65.1804 - FNR: 2.0926\n",
      "Epoch 35/50\n",
      "805/805 [==============================] - 0s 156us/step - loss: 0.3330 - acc: 0.8484 - FPR: 66.1338 - FNR: 1.2689\n",
      "Epoch 36/50\n",
      "805/805 [==============================] - 0s 145us/step - loss: 0.3356 - acc: 0.8534 - FPR: 67.0645 - FNR: 1.1298\n",
      "Epoch 37/50\n",
      "805/805 [==============================] - 0s 126us/step - loss: 0.3341 - acc: 0.8460 - FPR: 65.4535 - FNR: 0.4259\n",
      "Epoch 38/50\n",
      "805/805 [==============================] - 0s 125us/step - loss: 0.3340 - acc: 0.8435 - FPR: 66.4818 - FNR: 1.5735\n",
      "Epoch 39/50\n",
      "805/805 [==============================] - 0s 123us/step - loss: 0.3371 - acc: 0.8522 - FPR: 65.4934 - FNR: 1.1594\n",
      "Epoch 40/50\n",
      "805/805 [==============================] - 0s 125us/step - loss: 0.3307 - acc: 0.8460 - FPR: 66.2452 - FNR: 0.9228\n",
      "Epoch 41/50\n",
      "805/805 [==============================] - 0s 129us/step - loss: 0.3338 - acc: 0.8559 - FPR: 66.8535 - FNR: 0.9435\n",
      "Epoch 42/50\n",
      "805/805 [==============================] - 0s 141us/step - loss: 0.3356 - acc: 0.8534 - FPR: 66.4236 - FNR: 1.4389\n",
      "Epoch 43/50\n",
      "805/805 [==============================] - 0s 173us/step - loss: 0.3326 - acc: 0.8509 - FPR: 65.6970 - FNR: 0.8917\n",
      "Epoch 44/50\n",
      "805/805 [==============================] - 0s 134us/step - loss: 0.3347 - acc: 0.8497 - FPR: 66.8954 - FNR: 1.1880\n",
      "Epoch 45/50\n",
      "805/805 [==============================] - 0s 126us/step - loss: 0.3335 - acc: 0.8435 - FPR: 66.2836 - FNR: 1.3576\n",
      "Epoch 46/50\n",
      "805/805 [==============================] - 0s 157us/step - loss: 0.3366 - acc: 0.8571 - FPR: 66.3679 - FNR: 1.1091\n",
      "Epoch 47/50\n",
      "805/805 [==============================] - 0s 176us/step - loss: 0.3356 - acc: 0.8522 - FPR: 66.8796 - FNR: 1.2984\n",
      "Epoch 48/50\n",
      "805/805 [==============================] - 0s 130us/step - loss: 0.3321 - acc: 0.8571 - FPR: 66.3837 - FNR: 2.7743\n",
      "Epoch 49/50\n",
      "805/805 [==============================] - 0s 136us/step - loss: 0.3363 - acc: 0.8435 - FPR: 66.0904 - FNR: 1.6405\n",
      "Epoch 50/50\n",
      "805/805 [==============================] - 0s 164us/step - loss: 0.3358 - acc: 0.8559 - FPR: 65.7103 - FNR: 0.8518\n"
     ]
    }
   ],
   "source": [
    "training = network.fit(x_train, y_train, epochs=50, batch_size=10, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
